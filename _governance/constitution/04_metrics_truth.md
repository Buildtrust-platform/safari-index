North Star & Metrics Constitution

(Truth · Discipline · Anti-Vanity)

1. PURPOSE OF THIS DOCUMENT

This document defines what success means, how it is measured, and what is deliberately ignored.

It exists to prevent:

false confidence from vanity metrics

growth that weakens judgment

feature success without decision success

AI optimization toward the wrong outcomes

If this document is ignored, the platform may grow — and still fail.

2. CORE PRINCIPLE (NON-NEGOTIABLE)

The platform succeeds only when it reduces decision burden and increases decision clarity.

Revenue, traffic, and engagement are consequences, not goals.

3. THE NORTH STAR METRIC (SINGLE SOURCE OF TRUTH)
Decision Completion Rate (DCR)

Definition

The percentage of engaged users who leave a meaningful interaction with a clear, recorded decision outcome:
Book / Wait / Switch / Discard

What “Engaged User” Means (Strict Definition)

A user is considered engaged only if they:

read a decision page beyond the verdict section, or

complete a decision tool input flow, or

generate or view an itinerary, or

request or receive a decision report

Casual page views do not qualify.

Why DCR is the North Star

DCR measures:

trust

usefulness

clarity

authority

responsibility transfer

If DCR improves:

sales resistance drops

regret drops

conversion quality improves

long-term credibility compounds

No other metric captures this.

4. VALID DECISION OUTCOMES (DICTIONARY)

A decision outcome must be explicitly recorded as one of the following:

Book
The user is ready to proceed now.

Wait
The user has clarity that waiting improves the outcome.

Switch
The user understands that a different option fits better.

Discard
The user understands this is not the right trip at this time.

All four outcomes are considered successful.

Any metric that treats “discard” as failure is invalid.

5. SUPPORTING METRICS (SECONDARY, DIAGNOSTIC)

These metrics explain why DCR moves, but never replace it.

5.1 Time to Decision (TTD)

Definition
Median time between first meaningful engagement and decision outcome.

Interpretation

Decreasing TTD = improved clarity

Increasing TTD = growing uncertainty or friction

TTD is not about speed for its own sake.
Short decisions are good only if clarity remains.

5.2 Decision Reversal Rate (DRR)

Definition
Percentage of users who return after a decision expressing renewed confusion.

Interpretation

High DRR = overconfident or unclear recommendations

Low DRR = durable decisions

This metric guards against premature conclusions.

5.3 Post-Decision Drop-off Rate (PDDR)

Definition
Percentage of users who disengage immediately after a decision without understanding next steps.

Interpretation

High PDDR = poor transition to action or reassurance

Low PDDR = strong stewardship

5.4 Decision Assurance Attachment Rate (DAAR)

Definition
Percentage of decisions accompanied by a Decision Assurance artifact.

Interpretation

High DAAR = high trust and perceived value

Low DAAR = either strong confidence or unclear value proposition

This metric informs monetization, not success.

6. METRICS WE INTENTIONALLY IGNORE (ABSOLUTE)

The following metrics must never be used to judge success in isolation:

Pageviews

Bounce rate

Session duration

Number of published pages

Tool usage counts

Click-through rates

Funnel conversion percentages without context

If any of these rise while DCR falls or stagnates, the platform is drifting.

7. METRIC HIERARCHY RULE

When metrics conflict:

DCR overrides all

Supporting metrics explain DCR

Vanity metrics are ignored

No exception.

8. HOW METRICS CONSTRAIN PRODUCT DECISIONS

A feature is considered successful only if it:

increases DCR, or

reduces Time to Decision without increasing reversal, or

improves decision durability

A feature that:

increases engagement

increases traffic

increases time on site

…but does not improve decisions is a failure.

It must be revised or removed.

9. HOW METRICS CONSTRAIN AI BEHAVIOR

AI systems must be evaluated by:

impact on DCR

impact on Decision Reversal Rate

clarity of recorded outcomes

AI is not allowed to optimize for:

conversation length

politeness

reassurance

“helpfulness” without resolution

An AI interaction that ends without a recorded decision is incomplete.

10. EDGE CASE METRIC COMPASS

When choosing between:

increasing conversion

increasing clarity

Choose clarity.

When choosing between:

reducing abandonment

enabling correct discard decisions

Choose correct discard.

When choosing between:

pleasing users

reducing regret

Choose regret reduction.

11. REVIEW CADENCE (METRIC GOVERNANCE)
Weekly

Track DCR trend

Flag anomalies

Monthly

Analyze supporting metrics

Identify decision friction patterns

Quarterly

Re-evaluate metric definitions

Ensure they still align with doctrine

Metrics evolve, but the North Star does not.

12. CROSS-REFERENCES

This document governs:

Feature prioritization

AI optimization

UX iteration

Monetization experiments

Roadmap decisions

It is constrained by:

Decision Philosophy & Responsibility Doctrine

UX & Task Flow Constitution

If conflict exists, Decision Doctrine wins.

13. FINAL LOCK STATEMENT

We do not measure success by how long users stay.
We measure it by how clearly they leave.
A discarded decision made with confidence is a success.
A booked decision made with doubt is not.

Status

LOCKED — Version 1.0

No roadmap, feature, or AI behavior may violate this constitution.